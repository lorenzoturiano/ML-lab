{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72154775",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This CCLE dataset is made by performing the usual bulk RNA-seq technique. It was then normalized using RPKM (Reads Per Kilobase of transcript per Million) to reduce variance in order to make samples comparable.\n",
    "\n",
    "The RPKM normalization solves two problems that are created during sequencing:\n",
    "\n",
    "1. **Sequencing depth**: it occurs when samples are sequenced differently, some samples may have been sequenced more or less than others. This effect does not reflect biological status, so we should correct for that.  \n",
    "2. **Gene length**: the gene length tells us how easy it is for that gene to be detected by sequencing machines. On one hand, longer genes are easier to detect, so their count will be bigger. On the other hand, smaller genes are more difficult. Normalizations on gene length take this into account.\n",
    "\n",
    "The normalised \\((i, j)\\) RPKM value where \\(i\\) is a gene and \\(j\\) is a sample:\n",
    "\n",
    "\\[\n",
    "\\mathrm{RPKM}_{i,j} \\;=\\; \\frac{x_{i,j}}{l_i \\,\\cdot\\, \\sum_j x_{i,j}} \\;\\times\\; 10^6\n",
    "\\]\n",
    "\n",
    "Where \\(x_{i,j}\\) is the raw count, \\(l_i\\) is gene length in kilobases (kb), and the denominator is total reads in sample \\(j\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------mRNA---------\n",
    "# Description:\n",
    "# Expression Data with gene as index and cells/samples as columns\n",
    "data_mrna_seq_rpkm = pd.read_csv('data_mrna_seq_rpkm.txt',\n",
    "                    sep = '\\t',\n",
    "                    comment = '#')\n",
    "\n",
    "data_mrna_seq_rpkm.set_index('Hugo_Symbol',inplace=True)\n",
    "\n",
    "# Merge with mean duplicated rows\n",
    "data_mrna_seq_rpkm = data_mrna_seq_rpkm.groupby(data_mrna_seq_rpkm.index).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8916bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mrna_seq_rpkm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8301ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------Mut query----------\n",
    "# Description:\n",
    "# Data telling  which sample is mutated and not-mutated\n",
    "mutations= pd.read_csv('mutations.txt',\n",
    "                    sep = '\\t',\n",
    "                    comment = '#')\n",
    "mutations.set_index('SAMPLE_ID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ae621",
   "metadata": {},
   "source": [
    "## Why predict on just the five Variant_Type classes\n",
    "\n",
    "We focus on SNP, DNP, ONP, INS, and DEL because:\n",
    "\n",
    "1. Biological clarity: these five labels describe the fundamental mutation mechanism, single-base or multi-base changes, insertions, and deletions, so the model learns clear patterns.  \n",
    "2. Balanced data: each Variant_Type occurs often enough to give the model enough examples, while the detailed Variant_Classification labels are very uneven and would leave some classes too small to learn.  \n",
    "3. Reduced complexity: Variant_Classification depends on gene structure and reading frame (for example a SNP can be silent or missense depending on codon), which our sequence‐only model cannot infer without extra annotation.  \n",
    "4. Modular workflow: once the model tags a variant as INS or DEL, we can apply separate rules or a second model to predict functional impact, keeping each step simpler and more reliable.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------Mut ALL-----------\n",
    "# Dataset containing mutation classes for all genes, it only contains mutated samples\n",
    "data_mutations = pd.read_csv('data_mutations.txt',\n",
    "                    sep = '\\t',\n",
    "                    comment = '#')\n",
    "\n",
    "# Extract TP53 from all genes\n",
    "data_mutations = data_mutations[data_mutations['Hugo_Symbol'] == 'TP53']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = data_mutations['Variant_Type'].unique()\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = data_mutations['Variant_Classification'].unique()\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted information\n",
    "data_mutations = data_mutations[['Tumor_Sample_Barcode', 'Variant_Type']]\n",
    "data_mutations.set_index('Tumor_Sample_Barcode', inplace=True)\n",
    "# There are repetitions of my mutation type (Variant_Type)\n",
    "# if there is the same sample with different Variant_Type it should be removed\n",
    "variant_check = data_mutations.groupby(data_mutations.index)[\"Variant_Type\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mutations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many mutations are in each Variant_Type\n",
    "counts = data_mutations['Variant_Type'].value_counts()\n",
    "\n",
    "# Print them one per line\n",
    "for variant_type, n in counts.items():\n",
    "    print(f\"{variant_type}: {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965665f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t = data_mrna_seq_rpkm.T\n",
    "\n",
    "data_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t.loc['253JBV_URINARY_TRACT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = {\"SNP\": 1, \"DNP\": 2, \"DEL\": 3, \"INS\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e0551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build target vector y\n",
    "y = []\n",
    "\n",
    "# prepare lists to collect rows and their sample name\n",
    "X_rows = []\n",
    "sample_names = []\n",
    "c = 0\n",
    "\n",
    "# iterate over each mutation record\n",
    "for bc, mut in data_mutations.iterrows():\n",
    "    # check if this barcode is in data_t’s index\n",
    "    if bc in data_t.index:\n",
    "        # grab the full row from data_t and store it\n",
    "        X_rows.append(data_t.loc[bc].values)\n",
    "        y.append(code[mut['Variant_Type']])\n",
    "        sample_names.append(bc)\n",
    "    else:\n",
    "        c += 1\n",
    "\n",
    "print(f\"Number of samples discarded: {c}\")\n",
    "\n",
    "# build a new DataFrame X from the collected rows\n",
    "X = pd.DataFrame(\n",
    "    X_rows,\n",
    "    index=sample_names,\n",
    "    columns=data_t.columns\n",
    ")\n",
    "\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c47f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f30bde",
   "metadata": {},
   "source": [
    "**Step 2: Train - Test split (80% - 20%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da468c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b0a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% train, 20% test, stratify to preserve class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train shape:\\n\\tX_train: {X_train.shape}\\n\\ty_train: {y_train.shape}\")\n",
    "print(f\"Test shape:\\n\\tX_test: {X_test.shape}\\n\\ty_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48156d5d",
   "metadata": {},
   "source": [
    "**Step 3: Model selection and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce43c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500,      # you can tune this\n",
    "    max_depth=None,        # full depth; you can limit for speed/regularization\n",
    "    random_state=42,\n",
    "    n_jobs=-1              # use all cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8246cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "n = y_test.shape[0]\n",
    "count_mispred = 0\n",
    "for i in range(n):\n",
    "    if y_test[i] != y_pred[i]:\n",
    "        count_mispred += 1\n",
    "\n",
    "# Compute the percentage of mispredictions (accuracy)\n",
    "percentage_mispred = count_mispred / n\n",
    "print(f'Accuracy: {(1-percentage_mispred)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-Test split\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "    X.values, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.long)\n",
    "X_test  = torch.tensor(X_test_np,  dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test_np,  dtype=torch.long)\n",
    "\n",
    "# Wrap in DataLoaders\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds  = TensorDataset(X_test,  y_test)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b070dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model: a small MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden=128, n_classes=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, n_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c072a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = MLP(in_features=X_train.shape[1]).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt     = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 5) training loop\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds  = model(xb)\n",
    "        loss   = loss_fn(preds, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241bef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total   = 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds  = model(xb).argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total   += yb.size(0)\n",
    "print(f\"Test accuracy: {correct/total:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
